## MPII Gaze Dataset â€“ Basic Structure & Preprocessing Guide

ğŸ“ Dataset Structure Overview

The dataset is organized like a folder tree. At the top level, everything is divided into different sections based on raw data, processed data, and annotations.

1ï¸âƒ£ Original Data (Raw Data)

Path example:

`Data â†’ Original â†’ p00 â†’ Day01

p00 â†’ Person ID (participant 00)

Day01 â†’ Data collected on Day 1`

Inside this folder:

ğŸ‘ Eye Patch Images

ğŸ‘ Cropped images of the left and right eyes.

These are the main inputs for gaze prediction models.

ğŸ“ Annotation

Each eye image has associated metadata:

ğŸ‘ *Detected eye landmarks*
Points around the eye (corners, eyelids, etc.).

ğŸ‘ *On-screen gaze target position (2D)*
The exact position on the screen where the person was looking.

ğŸ‘ *3D gaze target position*
Gaze position in 3D space.

ğŸ‘ *Estimated 3D head pose*
Direction and rotation of the head.

So basically, for each image you get:

Eye Image + Gaze Location + Head Direction
2ï¸âƒ£ Calibration Data

This is mainly used if you want precise 3D gaze estimation or screen coordinate mapping.

3ï¸âƒ£ Normalized Eye Images

4ï¸âƒ£ 5-Point Based 3D Face Model

A simplified 3D face model using 5 key facial landmarks.

Used for:

ğŸ‘ Head pose estimation

ğŸ‘ 3D alignment

5ï¸âƒ£ Evaluation Subset

Contains:

Image list only

Used for:

Testing and benchmarking models



